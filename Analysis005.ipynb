{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade --no-cache-dir https://get.graphlab.com/GraphLab-Create/2.1/vinicius.george@sga.pucminas.br/ABD5-4359-8FC7-5A22-091C-127B-C46B-F481/GraphLab-Create-License.tar.gz\n",
    "\n",
    "# An√°lise de sentimentos em Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all-corpora'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all-corpora\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeiramente, vamos importar as bibliotecas que ser√£o utilizadas.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "\n",
    "nltk.download('all-corpora')\n",
    "\n",
    "# teremos uma base composta por 6 sentimentos (labels): Alegria, Medo, Raiva, Tristeza, Surpresa e Nojo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treinamento = [\n",
    "    ('este trabalho e agrad√°vel','alegria'),\n",
    "    ('gosto de ficar no seu aconchego','alegria'),\n",
    "    ('fiz a ades√£o ao curso hoje porque eu gostei','alegria'),\n",
    "    ('eu sou admirada por muitos','alegria'),\n",
    "    ('adoro como voc√™ e','alegria'),\n",
    "    ('adoro seu cabelo macio','alegria'),\n",
    "    ('adoro a cor dos seus olhos','alegria'),\n",
    "    ('somos t√£o am√°veis um com o outro','alegria'),\n",
    "    ('sinto uma grande afei√ß√£o por ele','alegria'),\n",
    "    ('quero agradar meus filhos','alegria')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplo_base = pd.DataFrame(base_treinamento)\n",
    "exemplo_base.columns = ['Frase', 'Sentimento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho base treinamento 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alegria    10\n",
       "Name: Sentimento, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('tamanho base treinamento {}'.format(exemplo_base.shape[0]))\n",
    "exemplo_base.Sentimento.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria    100.0\n",
      "Name: Sentimento, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((exemplo_base.Sentimento.value_counts() / exemplo_base.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frase</th>\n",
       "      <th>Sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gosto de ficar no seu aconchego</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adoro como voc√™ e</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adoro seu cabelo macio</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>somos t√£o am√°veis um com o outro</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eu sou admirada por muitos</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adoro a cor dos seus olhos</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>este trabalho e agrad√°vel</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiz a ades√£o ao curso hoje porque eu gostei</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>quero agradar meus filhos</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sinto uma grande afei√ß√£o por ele</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Frase Sentimento\n",
       "1              gosto de ficar no seu aconchego    alegria\n",
       "4                            adoro como voc√™ e    alegria\n",
       "5                       adoro seu cabelo macio    alegria\n",
       "7             somos t√£o am√°veis um com o outro    alegria\n",
       "3                   eu sou admirada por muitos    alegria\n",
       "6                   adoro a cor dos seus olhos    alegria\n",
       "0                    este trabalho e agrad√°vel    alegria\n",
       "2  fiz a ades√£o ao curso hoje porque eu gostei    alegria\n",
       "9                    quero agradar meus filhos    alegria\n",
       "8             sinto uma grande afei√ß√£o por ele    alegria"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temos um conjunto relativamente bem balanceado, com refer√™ncia ao label (sentimento).\n",
    "\n",
    "# Vamos gerar uma amostra de 20 frases para ter uma ideia sobre o que iremos trabalhar.\n",
    "\n",
    "exemplo_base.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_teste = [\n",
    "    ('n√£o precisei pagar o ingresso','alegria'),\n",
    "    ('se eu ajeitar tudo fica bem','alegria'),\n",
    "    ('minha fortuna ultrapassa a sua','alegria'),\n",
    "    ('sou muito afortunado','alegria'),\n",
    "    ('e beneficio para todos esta nova medida','alegria'),\n",
    "    ('ficou lindo','alegria'),\n",
    "    ('achei esse sapato muito simp√°tico','alegria'),\n",
    "    ('estou ansiosa pela a sua chegada','alegria'),\n",
    "    ('congratula√ß√µes pelo seu anivers√°rio','alegria'),\n",
    "    ('delicadadamente ele a colocou para dormir','alegria'),\n",
    "    ('a musica e linda','alegria'),\n",
    "    ('sem musica eu n√£o vivo','alegria')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de Teste 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alegria    12\n",
       "Name: Sentimento, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplo_base_teste = pd.DataFrame(base_teste)\n",
    "exemplo_base_teste.columns = ['Frase', 'Sentimento']\n",
    "print('Tamanho da base de Teste {}'.format(exemplo_base_teste.shape[0]))\n",
    "exemplo_base_teste.Sentimento.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria    100.0\n",
      "Name: Sentimento, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((exemplo_base_teste.Sentimento.value_counts() / exemplo_base_teste.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords s√£o palavras comuns que normalmente n√£o contribuem para o significado de uma frase, pelo menos com rela√ß√£o ao prop√≥sito da informa√ß√£o e do processamento da linguagem natural.\n",
    "# S√£o palavras como ‚ÄúThe‚Äù e ‚Äúa‚Äù ((em ingl√™s) ou ‚ÄúO/A‚Äù e ‚ÄúUm/Uma‚Äù ((em portugu√™s).\n",
    "\n",
    "# Muitos mecanismos de busca filtram estas palavras (stopwords), como forma de economizar espa√ßo em seus √≠ndices de pesquisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['de', 'a', 'o', 'que', 'e', '√©', 'do', 'da', 'em', 'um', 'para',\n",
       "       'com', 'n√£o', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as',\n",
       "       'dos', 'como', 'mas', 'ao', 'ele', 'das', '√†', 'seu', 'sua', 'ou',\n",
       "       'quando', 'muito', 'nos', 'j√°', 'eu', 'tamb√©m', 's√≥', 'pelo',\n",
       "       'pela', 'at√©', 'isso', 'ela', 'entre', 'depois', 'sem', 'mesmo',\n",
       "       'aos', 'seus', 'quem', 'nas', 'me', 'esse', 'eles', 'voc√™', 'essa',\n",
       "       'num', 'nem', 'suas', 'meu', '√†s', 'minha', 'numa', 'pelos',\n",
       "       'elas', 'qual', 'n√≥s', 'lhe', 'deles', 'essas', 'esses', 'pelas',\n",
       "       'este', 'dele', 'tu', 'te', 'voc√™s', 'vos', 'lhes', 'meus',\n",
       "       'minhas', 'teu', 'tua', 'teus', 'tuas', 'nosso', 'nossa', 'nossos',\n",
       "       'nossas', 'dela', 'delas', 'esta', 'estes', 'estas', 'aquele',\n",
       "       'aquela', 'aqueles', 'aquelas', 'isto', 'aquilo', 'estou', 'est√°',\n",
       "       'estamos', 'est√£o', 'estive', 'esteve', 'estivemos', 'estiveram',\n",
       "       'estava', 'est√°vamos', 'estavam', 'estivera', 'estiv√©ramos',\n",
       "       'esteja', 'estejamos', 'estejam', 'estivesse', 'estiv√©ssemos',\n",
       "       'estivessem', 'estiver', 'estivermos', 'estiverem', 'hei', 'h√°',\n",
       "       'havemos', 'h√£o', 'houve', 'houvemos', 'houveram', 'houvera',\n",
       "       'houv√©ramos', 'haja', 'hajamos', 'hajam', 'houvesse',\n",
       "       'houv√©ssemos', 'houvessem', 'houver', 'houvermos', 'houverem',\n",
       "       'houverei', 'houver√°', 'houveremos', 'houver√£o', 'houveria',\n",
       "       'houver√≠amos', 'houveriam', 'sou', 'somos', 's√£o', 'era', '√©ramos',\n",
       "       'eram', 'fui', 'foi', 'fomos', 'foram', 'fora', 'f√¥ramos', 'seja',\n",
       "       'sejamos', 'sejam', 'fosse', 'f√¥ssemos', 'fossem', 'for', 'formos',\n",
       "       'forem', 'serei', 'ser√°', 'seremos', 'ser√£o', 'seria', 'ser√≠amos',\n",
       "       'seriam', 'tenho', 'tem', 'temos', 't√©m', 'tinha', 't√≠nhamos',\n",
       "       'tinham', 'tive', 'teve', 'tivemos', 'tiveram', 'tivera',\n",
       "       'tiv√©ramos', 'tenha', 'tenhamos', 'tenham', 'tivesse',\n",
       "       'tiv√©ssemos', 'tivessem', 'tiver', 'tivermos', 'tiverem', 'terei',\n",
       "       'ter√°', 'teremos', 'ter√£o', 'teria', 'ter√≠amos', 'teriam'],\n",
       "      dtype='<U12')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_Stop = nltk.corpus.stopwords.words('portuguese')\n",
    "np.transpose(lista_Stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essas s√£o as StopWords que j√° s√£o contempladas na Biblioteca do NLTK.\n",
    "\n",
    "# Se o conjunto de palavras contido no Corpus do StopWords n√£o lhe atender, voc√™ pode facilmente adicionar mais palavras que fazem sentido para seu modelo e/ou at√© mesmo criar sua pr√≥pria lista de StopWords customizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclus√£o de algumas StopWords para exemplificar a facilidade de trabalhar com NLTK\n",
    "lista_Stop.append('tipo')\n",
    "lista_Stop.append('t√£o')\n",
    "lista_Stop.append('tudo')\n",
    "lista_Stop.append('vai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora podemos criar nossa fun√ß√£o para retirar as StopWords do nosso Corpus\n",
    "\n",
    "def removeStopWords(texto):\n",
    "    frases = []\n",
    "    for (palavras, sentimento) in texto:\n",
    "        # Criamos um list compreheension para extrair apenas as palavras que n√£o est√£o na Lista_Stop\n",
    "        semStop = [ p for p in palavras.split() if p not in lista_Stop]\n",
    "        # Inserindo as frases com os Labels (sentimento) j√° tratadas pela Lista_Stop\n",
    "        frases.append((semStop, sentimento))\n",
    "    return frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "# Stemming √© a t√©cnica de remover sufixos e prefixos de uma palavra, chamada stem.\n",
    "\n",
    "# Por exemplo, o stem da palavra cooking √© cook. Um bom algoritmo sabe que ‚Äúing‚Äù √© um sufixo e pode ser removido.\n",
    "\n",
    "# Stemming √© muito usado em mecanismos de buscas para indexa√ß√£o de palavras.\n",
    "\n",
    "# Ao inv√©s de armazenar todas as formas de uma palavras, um mecamismo de busca armazena apenas o stem da palavra, reduzindo o tamanho do √≠ndice e aumentando a performance do processo de busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rslp')\n",
    "\n",
    "def aplica_Stemmer(texto):\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    # Escolhido o RSLPS pois √© especifico da lingua portuguesa\n",
    "    frases_sem_Stemming = []\n",
    "    for (palavras, sentimento) in texto:\n",
    "        com_Stemming = [str(stemmer.stem(p)) for p in palavras.split() if p not in lista_Stop]\n",
    "        frases_sem_Stemming.append((com_Stemming, sentimento))\n",
    "    return frases_sem_Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frase</th>\n",
       "      <th>Sentimento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ador, cabel, maci]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[quer, agrad, filh]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[am, outr]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[trabalh, agrad]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[admir, muit]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[sint, grand, afe]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ador, cor, olh]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fiz, ades, curs, hoj, porqu, gost]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ador]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[gost, fic, aconcheg]</td>\n",
       "      <td>alegria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Frase Sentimento\n",
       "5                  [ador, cabel, maci]    alegria\n",
       "9                  [quer, agrad, filh]    alegria\n",
       "7                           [am, outr]    alegria\n",
       "0                     [trabalh, agrad]    alegria\n",
       "3                        [admir, muit]    alegria\n",
       "8                   [sint, grand, afe]    alegria\n",
       "6                     [ador, cor, olh]    alegria\n",
       "2  [fiz, ades, curs, hoj, porqu, gost]    alegria\n",
       "4                               [ador]    alegria\n",
       "1                [gost, fic, aconcheg]    alegria"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases_com_Stem_treinamento = aplica_Stemmer(base_treinamento)\n",
    "pd.DataFrame(frases_com_Stem_treinamento, columns=['Frase', 'Sentimento']).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_com_Stem_teste = aplica_Stemmer(base_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_Palavras(frases):\n",
    "    todas_Palavras = []\n",
    "    for (palavras, sentimento) in frases:\n",
    "        todas_Palavras.extend(palavras)\n",
    "    return todas_Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_treinamento = busca_Palavras(frases_com_Stem_treinamento)\n",
    "palavras_teste = busca_Palavras(frases_com_Stem_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de palavras na base de treinamento 0    28\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade de palavras na base de treinamento {}\".format(pd.DataFrame(palavras_treinamento).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_frequencia(palavras):\n",
    "    palavras = nltk.FreqDist(palavras)\n",
    "    return palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia_treinamento = busca_frequencia(palavras_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fun√ß√£o most_common do NLTK possibilita visualizar quais as palavras que ocorrem com maior frequ√™ncia em nosso texto.\n",
    "# Abaixo listamos apenas as 20 mais comuns. \n",
    "# Esta atividade pode nos ajudar a voltar a lista de StopWords e inserir novas palavras para serem excluidas de nossa an√°lise‚Ä¶\n",
    "# O trabalho do cientista de dados √© iterativo, sempre buscamos descobrir tend√™ncias, formas de melhorar o modelo, etc‚Ä¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ador', 3),\n",
       " ('agrad', 2),\n",
       " ('gost', 2),\n",
       " ('trabalh', 1),\n",
       " ('fic', 1),\n",
       " ('aconcheg', 1),\n",
       " ('fiz', 1),\n",
       " ('ades', 1),\n",
       " ('curs', 1),\n",
       " ('hoj', 1),\n",
       " ('porqu', 1),\n",
       " ('admir', 1),\n",
       " ('muit', 1),\n",
       " ('cabel', 1),\n",
       " ('maci', 1),\n",
       " ('cor', 1),\n",
       " ('olh', 1),\n",
       " ('am', 1),\n",
       " ('outr', 1),\n",
       " ('sint', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia_treinamento.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executamos tamb√©m para a base de treinamento\n",
    "frequencia_teste = busca_frequencia(palavras_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fun√ß√£o para retornar somente as palavras √∫nicas\n",
    "\n",
    "def busca_palavras_unicas(frequencia):\n",
    "    freq = frequencia.keys()\n",
    "    return freq\n",
    "\n",
    "palavras_unicas_treinamento = busca_palavras_unicas(frequencia_treinamento)\n",
    "palavras_unicas_teste = busca_palavras_unicas(frequencia_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precisamos agora criar uma fun√ß√£o para identificar quais palavras √∫nicas est√£o no documento passado para a fun√ß√£o!\n",
    "\n",
    "def extrator_palavras(documento):\n",
    "    # Utilizado set() para associar a variavel doc com o par√¢metro que esta chegando\n",
    "    doc = set(documento)\n",
    "    caracteristicas = {}\n",
    "    for palavras in palavras_unicas_treinamento:\n",
    "        caracteristicas['%s' % palavras] = (palavras in doc)\n",
    "    return caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devido a necessidade de aplica√ß√£o da fun√ß√£o Extrator_Palavras sobre as bases de Treinamento e Teste, como a vari√°vel palavras_unicas_teste exige a aplica√ß√£o isolada, precisamos criar uma fun√ß√£o apartada somente para a base de teste!\n",
    "\n",
    "def extrator_palavras_teste(documento):\n",
    "    doc = set(documento)\n",
    "    caracteristicas = {}\n",
    "    for palavras in palavras_unicas_teste:\n",
    "        caracteristicas['%s' % palavras] = (palavras in doc)\n",
    "    return caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o apply features do NLTK faz o preenchimento se tem ou n√£o (True / False) a caracter√≠stica de acordo do com o par√¢metro!\n",
    "\n",
    "base_completa_treinamento = nltk.classify.apply_features(extrator_palavras, frases_com_Stem_treinamento)\n",
    "base_completa_teste = nltk.classify.apply_features(extrator_palavras_teste, frases_com_Stem_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O algortimo NaiveBayes monta a tabela de probabilidade!\n",
    "\n",
    "classificador = nltk.NaiveBayesClassifier.train(base_completa_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alegria']\n"
     ]
    }
   ],
   "source": [
    "print(classificador.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classificador.show_most_informative_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Agora vamos verificar a acur√°cia do modelo!\n",
    "\n",
    "print(nltk.classify.accuracy(classificador, base_completa_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ter uma vis√£o mais clara dos erros e sentimentos que est√£o em conflito no nosso modelo, vamos gerar uma matriz de confus√£o do NLTK.\n",
    "\n",
    "erros = []\n",
    "for (frase, classe) in base_completa_teste:\n",
    "    #print(frase)\n",
    "    #print(classe)\n",
    "    resultado = classificador.classify(frase)\n",
    "    if resultado != classe:\n",
    "        erros.append((classe, resultado, frase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        |  a |\n",
      "        |  l |\n",
      "        |  e |\n",
      "        |  g |\n",
      "        |  r |\n",
      "        |  i |\n",
      "        |  a |\n",
      "--------+----+\n",
      "alegria |<12>|\n",
      "--------+----+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "esperado = []\n",
    "previsto = []\n",
    "for (frase, classe) in base_completa_teste:\n",
    "    resultado = classificador.classify(frase)\n",
    "    previsto.append(resultado)\n",
    "    esperado.append(classe)\n",
    "\n",
    "matriz = ConfusionMatrix(esperado, previsto)\n",
    "print(matriz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Para realmente saber como nosso modelo esta atuando na an√°lise de sentimento, vamos incluir algumas frases aleat√≥rias e verificar qual tag ele marca nossa frase!\n",
    "# teste = ‚ÄòNossa, que not√≠cia maravilhosa!‚Äô\n",
    "\n",
    "teste = 'Nossa, que not√≠cia maravilhosa!'\n",
    "testeStemming = []\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "for (palavras_treinamento) in teste.split():\n",
    "    comStem = [p for p in  palavras_treinamento.split()]\n",
    "    testeStemming.append(str(stemmer.stem(comStem[0])))\n",
    "\n",
    "novo = extrator_palavras(testeStemming)\n",
    "\n",
    "#print(classificador.classify(novo))\n",
    "distribuicao = classificador.prob_classify(novo)\n",
    "for classe in distribuicao.samples():\n",
    "    print('%s: %f' % (classe, distribuicao.prob(classe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Nosso modelo se comportou corretamente, identificou a frase como ‚ÄúSurpresa‚Äù, com 42%, bem acima dos demais sentimentos!!!\n",
    "# Vamos testar mais uma frase!\n",
    "# teste = ‚ÄòPqp, que tr√¢nsito chato da porra!‚Äô\n",
    "\n",
    "teste = 'Pqp, que tr√¢nsito chato da porra!'\n",
    "testeStemming = []\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "for (palavras_treinamento) in teste.split():\n",
    "    comStem = [p for p in  palavras_treinamento.split()]\n",
    "    testeStemming.append(str(stemmer.stem(comStem[0])))\n",
    "\n",
    "novo = extrator_palavras(testeStemming)\n",
    "\n",
    "#print(classificador.classify(novo))\n",
    "distribuicao = classificador.prob_classify(novo)\n",
    "for classe in distribuicao.samples():\n",
    "    print('%s: %f' % (classe, distribuicao.prob(classe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neste trabalho foi utilizada uma base de dados pequena (646 frases em treino e 256 frases em teste).\n",
    "\n",
    "# A an√°lise de sentimento pode ser utilizada em redes sociais para identificar o que os clientes est√£o falando de uma determinada marca, assunto, etc‚Ä¶ Bastaria conectar-se a uma API, buscar as informa√ß√µes de forma online e realizar a an√°lise de sentimento simultaneamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
